{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron(max_iter=1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor(a, b):\n",
    "    return (a and not b) or (not a and b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = True, True\n",
    "xor(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = False, False\n",
    "xor(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = True, False\n",
    "xor(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = False, True\n",
    "xor(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.00, NNZs: 0, Bias: 0.000000, T: 4, Avg. loss: 1.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.00, NNZs: 0, Bias: 0.000000, T: 8, Avg. loss: 1.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.00, NNZs: 0, Bias: 1.000000, T: 12, Avg. loss: 0.750000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.00, NNZs: 0, Bias: 1.000000, T: 16, Avg. loss: 1.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.00, NNZs: 1, Bias: 0.000000, T: 20, Avg. loss: 0.750000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.00, NNZs: 1, Bias: 1.000000, T: 24, Avg. loss: 0.500000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.00, NNZs: 1, Bias: 1.000000, T: 28, Avg. loss: 1.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.41, NNZs: 2, Bias: 0.000000, T: 32, Avg. loss: 0.750000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 36, Avg. loss: 0.750000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 40, Avg. loss: 1.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 44, Avg. loss: 1.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 48, Avg. loss: 1.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 52, Avg. loss: 1.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 56, Avg. loss: 1.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 60, Avg. loss: 1.000000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 64, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 68, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 72, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 76, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 80, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 84, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 88, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 92, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 96, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 100, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 104, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 108, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 112, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 116, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 120, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 124, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 128, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 132, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 136, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 140, Avg. loss: 1.000000\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 144, Avg. loss: 1.000000\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 148, Avg. loss: 1.000000\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 152, Avg. loss: 1.000000\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 156, Avg. loss: 1.000000\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 160, Avg. loss: 1.000000\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 164, Avg. loss: 1.000000\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 168, Avg. loss: 1.000000\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 172, Avg. loss: 1.000000\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 176, Avg. loss: 1.000000\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 180, Avg. loss: 1.000000\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 184, Avg. loss: 1.000000\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 188, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 192, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 196, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 200, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 204, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 208, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 212, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 216, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 220, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 224, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 228, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 232, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 236, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 240, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 244, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 248, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 252, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 256, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 260, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 264, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 268, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 272, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 276, Avg. loss: 1.000000\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 280, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 284, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 288, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 292, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 296, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 300, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 304, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 308, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 312, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 316, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 320, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 324, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 328, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 332, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 336, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 340, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 344, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 348, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 352, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 356, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 360, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 364, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 368, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 372, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 376, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 380, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 384, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 388, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 392, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 396, Avg. loss: 1.000000\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 400, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 404, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 408, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 412, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 416, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 420, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 424, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 428, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 432, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 436, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 440, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 444, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 448, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 452, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 456, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 460, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 464, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 468, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 472, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 476, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 480, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 484, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 488, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 492, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 496, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 500, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 504, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 508, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 512, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 516, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 520, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 524, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 528, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 532, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 536, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 540, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 544, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 548, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 552, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 556, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 560, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 564, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 568, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 572, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 576, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 580, Avg. loss: 1.000000\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 584, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 588, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 592, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 596, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 600, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 604, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 608, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 612, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 616, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 620, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 624, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 628, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 632, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 636, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 640, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 644, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 648, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 652, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 656, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 660, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 664, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 668, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 672, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 676, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 680, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 684, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 688, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 692, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 696, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 700, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 704, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 708, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 712, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 716, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 720, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 724, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 728, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 732, Avg. loss: 1.000000\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 736, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 740, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 744, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 748, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 752, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 756, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 760, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 764, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 768, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 772, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 776, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 780, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 784, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 788, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 792, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 796, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 800, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 804, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 808, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 812, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 816, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 820, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 824, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 828, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 832, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 836, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 840, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 844, Avg. loss: 1.000000\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 848, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 852, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 856, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 860, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 864, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 868, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 872, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 876, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 880, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 884, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 888, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 892, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 896, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 900, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 904, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 908, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 912, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 916, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 920, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 924, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 928, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 932, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 936, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 940, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 944, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 948, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 952, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 956, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 960, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 964, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 968, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 972, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 976, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 980, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 984, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 988, Avg. loss: 1.000000\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 992, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 996, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1000, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1004, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1008, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1012, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1016, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1020, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1024, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1028, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1032, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1036, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1040, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1044, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1048, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1052, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1056, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1060, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1064, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1068, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1072, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1076, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1080, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1084, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1088, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1092, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1096, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1100, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1104, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1108, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1112, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1116, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1120, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1124, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1128, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1132, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1136, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1140, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1144, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1148, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1152, Avg. loss: 1.000000\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1156, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 290\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1160, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 291\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1164, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1168, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1172, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1176, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1180, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1184, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 297\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1188, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1192, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1196, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1200, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 301\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1204, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 302\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1208, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 303\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1212, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 304\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1216, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 305\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1220, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 306\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1224, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 307\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1228, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 308\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1232, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 309\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1236, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 310\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1240, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 311\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1244, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 312\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1248, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 313\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1252, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 314\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1256, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 315\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1260, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 316\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1264, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 317\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1268, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 318\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1272, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 319\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1276, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 320\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1280, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 321\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1284, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 322\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1288, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 323\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1292, Avg. loss: 1.000000\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 324\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1296, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 325\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1300, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 326\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1304, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 327\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1308, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 328\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1312, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 329\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1316, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 330\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1320, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 331\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1324, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 332\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1328, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 333\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1332, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 334\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1336, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 335\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1340, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 336\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1344, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 337\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1348, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 338\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1352, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 339\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1356, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 340\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1360, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 341\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1364, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 342\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1368, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 343\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1372, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 344\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1376, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 345\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1380, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 346\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1384, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 347\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1388, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 348\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1392, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 349\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1396, Avg. loss: 1.000000\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 350\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1400, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 351\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1404, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 352\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1408, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 353\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1412, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 354\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1416, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 355\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1420, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 356\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1424, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 357\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1428, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 358\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1432, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 359\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1436, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 360\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1440, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 361\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1444, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 362\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1448, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 363\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1452, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 364\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1456, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 365\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1460, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 366\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1464, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 367\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1468, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 368\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1472, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 369\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1476, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 370\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1480, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 371\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1484, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 372\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1488, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 373\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1492, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 374\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1496, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 375\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1500, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 376\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1504, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 377\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1508, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 378\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1512, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 379\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1516, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 380\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1520, Avg. loss: 1.000000\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 381\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1524, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 382\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1528, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 383\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1532, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 384\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1536, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 385\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1540, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 386\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1544, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 387\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1548, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 388\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1552, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 389\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1556, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 390\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1560, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 391\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1564, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 392\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1568, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 393\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1572, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 394\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1576, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 395\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1580, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 396\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1584, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 397\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1588, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 398\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1592, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 399\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1596, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 400\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1600, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 401\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1604, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 402\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1608, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 403\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1612, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 404\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1616, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 405\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1620, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 406\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1624, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 407\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1628, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 408\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1632, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 409\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1636, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 410\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1640, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 411\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1644, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 412\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1648, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 413\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1652, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 414\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1656, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 415\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1660, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 416\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1664, Avg. loss: 1.000000\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 417\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1668, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 418\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1672, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 419\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1676, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 420\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1680, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 421\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1684, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 422\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1688, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 423\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1692, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 424\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1696, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 425\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1700, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 426\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1704, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 427\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1708, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 428\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1712, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 429\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1716, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 430\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1720, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 431\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1724, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 432\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1728, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 433\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1732, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 434\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1736, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 435\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1740, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 436\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1744, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 437\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1748, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 438\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1752, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 439\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1756, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 440\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1760, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 441\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1764, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 442\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1768, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 443\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1772, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 444\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1776, Avg. loss: 1.000000\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 445\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1780, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 446\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1784, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 447\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1788, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 448\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1792, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 449\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1796, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 450\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1800, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 451\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1804, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 452\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1808, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 453\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1812, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 454\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1816, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 455\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1820, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 456\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1824, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 457\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1828, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 458\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1832, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 459\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1836, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 460\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1840, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 461\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1844, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 462\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1848, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 463\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1852, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 464\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1856, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 465\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1860, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 466\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1864, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 467\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1868, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 468\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1872, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 469\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1876, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 470\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1880, Avg. loss: 1.000000\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 471\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1884, Avg. loss: 1.000000\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 472\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1888, Avg. loss: 1.000000\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 473\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1892, Avg. loss: 1.000000\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 474\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1896, Avg. loss: 1.000000\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 475\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1900, Avg. loss: 1.000000\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 476\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1904, Avg. loss: 1.000000\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 477\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1908, Avg. loss: 1.000000\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 478\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1912, Avg. loss: 1.000000\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 479\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1916, Avg. loss: 1.000000\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 480\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1920, Avg. loss: 1.000000\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 481\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1924, Avg. loss: 1.000000\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 482\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1928, Avg. loss: 1.000000\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 483\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1932, Avg. loss: 1.000000\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 484\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1936, Avg. loss: 1.000000\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 485\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1940, Avg. loss: 1.000000\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 486\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1944, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 487\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1948, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 488\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1952, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 489\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1956, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 490\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1960, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 491\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1964, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 492\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1968, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 493\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1972, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 494\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1976, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 495\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1980, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 496\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1984, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 497\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1988, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 498\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1992, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 499\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 1996, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 500\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2000, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 501\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2004, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 502\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2008, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 503\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2012, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 504\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2016, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 505\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2020, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 506\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2024, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 507\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2028, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 508\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2032, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 509\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2036, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 510\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2040, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 511\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2044, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 512\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2048, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 513\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2052, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 514\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2056, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 515\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2060, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 516\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2064, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 517\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2068, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 518\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2072, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 519\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2076, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 520\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2080, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 521\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2084, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 522\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2088, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 523\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2092, Avg. loss: 1.000000\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 524\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2096, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 525\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2100, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 526\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2104, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 527\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2108, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 528\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2112, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 529\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2116, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 530\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2120, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 531\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2124, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 532\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2128, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 533\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2132, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 534\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2136, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 535\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2140, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 536\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2144, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 537\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2148, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 538\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2152, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 539\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2156, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 540\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2160, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 541\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2164, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 542\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2168, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 543\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2172, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 544\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2176, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 545\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2180, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 546\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2184, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 547\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2188, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 548\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2192, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 549\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2196, Avg. loss: 1.000000\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 550\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2200, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 551\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2204, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 552\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2208, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 553\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2212, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 554\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2216, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 555\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2220, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 556\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2224, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 557\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2228, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 558\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2232, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 559\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2236, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 560\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2240, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 561\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2244, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 562\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2248, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 563\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2252, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 564\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2256, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 565\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2260, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 566\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2264, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 567\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2268, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 568\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2272, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 569\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2276, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 570\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2280, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 571\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2284, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 572\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2288, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 573\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2292, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 574\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2296, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 575\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2300, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 576\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2304, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 577\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2308, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 578\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2312, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 579\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2316, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 580\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2320, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 581\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2324, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 582\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2328, Avg. loss: 1.000000\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 583\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2332, Avg. loss: 1.000000\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 584\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2336, Avg. loss: 1.000000\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 585\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2340, Avg. loss: 1.000000\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 586\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2344, Avg. loss: 1.000000\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 587\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2348, Avg. loss: 1.000000\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 588\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2352, Avg. loss: 1.000000\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 589\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2356, Avg. loss: 1.000000\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 590\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2360, Avg. loss: 1.000000\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 591\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2364, Avg. loss: 1.000000\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 592\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2368, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 593\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2372, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 594\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2376, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 595\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2380, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 596\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2384, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 597\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2388, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 598\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2392, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 599\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2396, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 600\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2400, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 601\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2404, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 602\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2408, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 603\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2412, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 604\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2416, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 605\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2420, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 606\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2424, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 607\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2428, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 608\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2432, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 609\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2436, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 610\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2440, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 611\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2444, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 612\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2448, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 613\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2452, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 614\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2456, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 615\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2460, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 616\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2464, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 617\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2468, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 618\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2472, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 619\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2476, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 620\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2480, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 621\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2484, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 622\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2488, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 623\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2492, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 624\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2496, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 625\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2500, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 626\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2504, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 627\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2508, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 628\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2512, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 629\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2516, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 630\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2520, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 631\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2524, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 632\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2528, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 633\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2532, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 634\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2536, Avg. loss: 1.000000\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 635\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2540, Avg. loss: 1.000000\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 636\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2544, Avg. loss: 1.000000\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 637\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2548, Avg. loss: 1.000000\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 638\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2552, Avg. loss: 1.000000\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 639\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2556, Avg. loss: 1.000000\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 640\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2560, Avg. loss: 1.000000\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 641\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2564, Avg. loss: 1.000000\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 642\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2568, Avg. loss: 1.000000\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 643\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2572, Avg. loss: 1.000000\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 644\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2576, Avg. loss: 1.000000\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 645\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2580, Avg. loss: 1.000000\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 646\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2584, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 647\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2588, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 648\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2592, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 649\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2596, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 650\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2600, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 651\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2604, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 652\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2608, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 653\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2612, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 654\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2616, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 655\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2620, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 656\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2624, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 657\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2628, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 658\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2632, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 659\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2636, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 660\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2640, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 661\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2644, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 662\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2648, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 663\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2652, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 664\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2656, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 665\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2660, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 666\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2664, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 667\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2668, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 668\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2672, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 669\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2676, Avg. loss: 1.000000\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 670\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2680, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 671\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2684, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 672\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2688, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 673\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2692, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 674\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2696, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 675\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2700, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 676\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2704, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 677\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2708, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 678\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2712, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 679\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2716, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 680\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2720, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 681\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2724, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 682\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2728, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 683\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2732, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 684\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2736, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 685\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2740, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 686\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2744, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 687\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2748, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 688\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2752, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 689\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2756, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 690\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2760, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 691\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2764, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 692\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2768, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 693\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2772, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 694\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2776, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 695\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2780, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 696\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2784, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 697\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2788, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 698\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2792, Avg. loss: 1.000000\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 699\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2796, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 700\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2800, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 701\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2804, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 702\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2808, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 703\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2812, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 704\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2816, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 705\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2820, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 706\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2824, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 707\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2828, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 708\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2832, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 709\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2836, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 710\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2840, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 711\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2844, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 712\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2848, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 713\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2852, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 714\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2856, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 715\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2860, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 716\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2864, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 717\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2868, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 718\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2872, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 719\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2876, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 720\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2880, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 721\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2884, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 722\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2888, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 723\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2892, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 724\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2896, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 725\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2900, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 726\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2904, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 727\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2908, Avg. loss: 1.000000\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 728\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2912, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 729\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2916, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 730\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2920, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 731\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2924, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 732\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2928, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 733\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2932, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 734\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2936, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 735\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2940, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 736\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2944, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 737\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2948, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 738\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2952, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 739\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2956, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 740\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2960, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 741\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2964, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 742\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2968, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 743\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2972, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 744\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2976, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 745\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2980, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 746\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2984, Avg. loss: 1.000000\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 747\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2988, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 748\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2992, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 749\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 2996, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 750\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3000, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 751\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3004, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 752\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3008, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 753\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3012, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 754\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3016, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 755\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3020, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 756\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3024, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 757\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3028, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 758\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3032, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 759\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3036, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 760\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3040, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 761\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3044, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 762\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3048, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 763\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3052, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 764\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3056, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 765\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3060, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 766\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3064, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 767\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3068, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 768\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3072, Avg. loss: 1.000000\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 769\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3076, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 770\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3080, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 771\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3084, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 772\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3088, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 773\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3092, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 774\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3096, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 775\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3100, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 776\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3104, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 777\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3108, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 778\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3112, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 779\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3116, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 780\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3120, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 781\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3124, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 782\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3128, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 783\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3132, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 784\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3136, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 785\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3140, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 786\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3144, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 787\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3148, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 788\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3152, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 789\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3156, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 790\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3160, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 791\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3164, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 792\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3168, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 793\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3172, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 794\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3176, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 795\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3180, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 796\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3184, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 797\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3188, Avg. loss: 1.000000\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 798\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3192, Avg. loss: 1.000000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 799\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3196, Avg. loss: 1.000000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 800\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3200, Avg. loss: 1.000000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 801\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3204, Avg. loss: 1.000000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 802\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3208, Avg. loss: 1.000000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 803\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3212, Avg. loss: 1.000000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 804\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3216, Avg. loss: 1.000000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 805\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3220, Avg. loss: 1.000000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 806\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3224, Avg. loss: 1.000000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 807\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3228, Avg. loss: 1.000000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 808\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3232, Avg. loss: 1.000000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 809\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3236, Avg. loss: 1.000000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 810\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3240, Avg. loss: 1.000000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 811\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3244, Avg. loss: 1.000000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 812\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3248, Avg. loss: 1.000000\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 813\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3252, Avg. loss: 1.000000\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 814\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3256, Avg. loss: 1.000000\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 815\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3260, Avg. loss: 1.000000\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 816\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3264, Avg. loss: 1.000000\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 817\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3268, Avg. loss: 1.000000\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 818\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3272, Avg. loss: 1.000000\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 819\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3276, Avg. loss: 1.000000\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 820\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3280, Avg. loss: 1.000000\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 821\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3284, Avg. loss: 1.000000\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 822\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3288, Avg. loss: 1.000000\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 823\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3292, Avg. loss: 1.000000\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 824\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3296, Avg. loss: 1.000000\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 825\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3300, Avg. loss: 1.000000\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 826\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3304, Avg. loss: 1.000000\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 827\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3308, Avg. loss: 1.000000\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 828\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3312, Avg. loss: 1.000000\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 829\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3316, Avg. loss: 1.000000\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 830\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3320, Avg. loss: 1.000000\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 831\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3324, Avg. loss: 1.000000\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 832\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3328, Avg. loss: 1.000000\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 833\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3332, Avg. loss: 1.000000\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 834\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3336, Avg. loss: 1.000000\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 835\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3340, Avg. loss: 1.000000\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 836\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3344, Avg. loss: 1.000000\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 837\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3348, Avg. loss: 1.000000\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 838\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3352, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 839\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3356, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 840\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3360, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 841\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3364, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 842\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3368, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 843\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3372, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 844\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3376, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 845\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3380, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 846\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3384, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 847\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3388, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 848\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3392, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 849\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3396, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 850\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3400, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 851\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3404, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 852\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3408, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 853\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3412, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 854\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3416, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 855\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3420, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 856\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3424, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 857\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3428, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 858\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3432, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 859\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3436, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 860\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3440, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 861\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3444, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 862\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3448, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 863\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3452, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 864\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3456, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 865\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3460, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 866\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3464, Avg. loss: 1.000000\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 867\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3468, Avg. loss: 1.000000\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 868\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3472, Avg. loss: 1.000000\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 869\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3476, Avg. loss: 1.000000\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 870\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3480, Avg. loss: 1.000000\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 871\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3484, Avg. loss: 1.000000\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 872\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3488, Avg. loss: 1.000000\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 873\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3492, Avg. loss: 1.000000\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 874\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3496, Avg. loss: 1.000000\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 875\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3500, Avg. loss: 1.000000\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 876\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3504, Avg. loss: 1.000000\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 877\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3508, Avg. loss: 1.000000\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 878\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3512, Avg. loss: 1.000000\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 879\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3516, Avg. loss: 1.000000\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 880\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3520, Avg. loss: 1.000000\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 881\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3524, Avg. loss: 1.000000\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 882\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3528, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 883\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3532, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 884\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3536, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 885\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3540, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 886\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3544, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 887\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3548, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 888\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3552, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 889\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3556, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 890\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3560, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 891\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3564, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 892\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3568, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 893\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3572, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 894\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3576, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 895\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3580, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 896\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3584, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 897\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3588, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 898\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3592, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 899\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3596, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 900\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3600, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 901\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3604, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 902\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3608, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 903\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3612, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 904\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3616, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 905\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3620, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 906\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3624, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 907\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3628, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 908\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3632, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 909\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3636, Avg. loss: 1.000000\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 910\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3640, Avg. loss: 1.000000\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 911\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3644, Avg. loss: 1.000000\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 912\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3648, Avg. loss: 1.000000\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 913\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3652, Avg. loss: 1.000000\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 914\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3656, Avg. loss: 1.000000\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 915\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3660, Avg. loss: 1.000000\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 916\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3664, Avg. loss: 1.000000\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 917\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3668, Avg. loss: 1.000000\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 918\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3672, Avg. loss: 1.000000\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 919\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3676, Avg. loss: 1.000000\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 920\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3680, Avg. loss: 1.000000\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 921\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3684, Avg. loss: 1.000000\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 922\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3688, Avg. loss: 1.000000\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 923\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3692, Avg. loss: 1.000000\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 924\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3696, Avg. loss: 1.000000\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 925\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3700, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 926\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3704, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 927\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3708, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 928\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3712, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 929\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3716, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 930\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3720, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 931\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3724, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 932\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3728, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 933\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3732, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 934\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3736, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 935\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3740, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 936\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3744, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 937\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3748, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 938\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3752, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 939\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3756, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 940\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3760, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 941\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3764, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 942\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3768, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 943\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3772, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 944\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3776, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 945\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3780, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 946\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3784, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 947\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3788, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 948\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3792, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 949\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3796, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 950\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3800, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 951\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3804, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 952\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3808, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 953\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3812, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 954\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3816, Avg. loss: 1.000000\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 955\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3820, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 956\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3824, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 957\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3828, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 958\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3832, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 959\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3836, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 960\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3840, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 961\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3844, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 962\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3848, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 963\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3852, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 964\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3856, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 965\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3860, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 966\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3864, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 967\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3868, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 968\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3872, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 969\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3876, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 970\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3880, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 971\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3884, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 972\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3888, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 973\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3892, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 974\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3896, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 975\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3900, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 976\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3904, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 977\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3908, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 978\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3912, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 979\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3916, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 980\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3920, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 981\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3924, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 982\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3928, Avg. loss: 1.000000\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 983\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3932, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 984\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3936, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 985\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3940, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 986\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3944, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 987\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3948, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 988\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3952, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 989\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3956, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 990\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3960, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 991\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3964, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 992\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3968, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 993\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3972, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 994\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3976, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 995\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3980, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 996\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3984, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 997\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3988, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 998\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3992, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 999\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 3996, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 1000\n",
      "Norm: 1.41, NNZs: 2, Bias: 1.000000, T: 4000, Avg. loss: 1.000000\n",
      "Total training time: 0.40 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
       "      fit_intercept=True, max_iter=1000, n_iter=None, n_iter_no_change=5,\n",
       "      n_jobs=None, penalty=None, random_state=0, shuffle=True, tol=None,\n",
       "      validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [[1, 1], [0, 0], [1, 0], [0, 1]]\n",
    "y_train = [0, 0, 1, 1]\n",
    "\n",
    "perceptron.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = perceptron.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlpc = MLPClassifier(hidden_layer_sizes=(10,), verbose=1, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69992813\n",
      "Iteration 2, loss = 0.69888226\n",
      "Iteration 3, loss = 0.69787826\n",
      "Iteration 4, loss = 0.69690170\n",
      "Iteration 5, loss = 0.69593614\n",
      "Iteration 6, loss = 0.69498105\n",
      "Iteration 7, loss = 0.69403613\n",
      "Iteration 8, loss = 0.69310110\n",
      "Iteration 9, loss = 0.69221020\n",
      "Iteration 10, loss = 0.69134497\n",
      "Iteration 11, loss = 0.69048757\n",
      "Iteration 12, loss = 0.68963743\n",
      "Iteration 13, loss = 0.68879403\n",
      "Iteration 14, loss = 0.68795702\n",
      "Iteration 15, loss = 0.68712614\n",
      "Iteration 16, loss = 0.68630122\n",
      "Iteration 17, loss = 0.68548214\n",
      "Iteration 18, loss = 0.68466883\n",
      "Iteration 19, loss = 0.68386123\n",
      "Iteration 20, loss = 0.68305929\n",
      "Iteration 21, loss = 0.68226298\n",
      "Iteration 22, loss = 0.68147227\n",
      "Iteration 23, loss = 0.68068711\n",
      "Iteration 24, loss = 0.67990747\n",
      "Iteration 25, loss = 0.67913333\n",
      "Iteration 26, loss = 0.67836463\n",
      "Iteration 27, loss = 0.67760134\n",
      "Iteration 28, loss = 0.67684342\n",
      "Iteration 29, loss = 0.67609083\n",
      "Iteration 30, loss = 0.67534352\n",
      "Iteration 31, loss = 0.67460144\n",
      "Iteration 32, loss = 0.67386455\n",
      "Iteration 33, loss = 0.67313279\n",
      "Iteration 34, loss = 0.67240611\n",
      "Iteration 35, loss = 0.67168445\n",
      "Iteration 36, loss = 0.67096775\n",
      "Iteration 37, loss = 0.67025595\n",
      "Iteration 38, loss = 0.66954900\n",
      "Iteration 39, loss = 0.66884682\n",
      "Iteration 40, loss = 0.66814935\n",
      "Iteration 41, loss = 0.66745653\n",
      "Iteration 42, loss = 0.66676828\n",
      "Iteration 43, loss = 0.66608454\n",
      "Iteration 44, loss = 0.66540524\n",
      "Iteration 45, loss = 0.66473030\n",
      "Iteration 46, loss = 0.66405964\n",
      "Iteration 47, loss = 0.66339320\n",
      "Iteration 48, loss = 0.66273090\n",
      "Iteration 49, loss = 0.66207266\n",
      "Iteration 50, loss = 0.66141840\n",
      "Iteration 51, loss = 0.66078914\n",
      "Iteration 52, loss = 0.66020792\n",
      "Iteration 53, loss = 0.65962412\n",
      "Iteration 54, loss = 0.65903868\n",
      "Iteration 55, loss = 0.65845239\n",
      "Iteration 56, loss = 0.65786740\n",
      "Iteration 57, loss = 0.65729511\n",
      "Iteration 58, loss = 0.65680102\n",
      "Iteration 59, loss = 0.65628986\n",
      "Iteration 60, loss = 0.65576426\n",
      "Iteration 61, loss = 0.65524161\n",
      "Iteration 62, loss = 0.65471181\n",
      "Iteration 63, loss = 0.65417518\n",
      "Iteration 64, loss = 0.65363294\n",
      "Iteration 65, loss = 0.65308617\n",
      "Iteration 66, loss = 0.65253868\n",
      "Iteration 67, loss = 0.65201358\n",
      "Iteration 68, loss = 0.65147998\n",
      "Iteration 69, loss = 0.65094216\n",
      "Iteration 70, loss = 0.65047225\n",
      "Iteration 71, loss = 0.64999346\n",
      "Iteration 72, loss = 0.64949848\n",
      "Iteration 73, loss = 0.64902115\n",
      "Iteration 74, loss = 0.64855244\n",
      "Iteration 75, loss = 0.64808347\n",
      "Iteration 76, loss = 0.64760948\n",
      "Iteration 77, loss = 0.64713121\n",
      "Iteration 78, loss = 0.64664931\n",
      "Iteration 79, loss = 0.64617159\n",
      "Iteration 80, loss = 0.64569794\n",
      "Iteration 81, loss = 0.64523611\n",
      "Iteration 82, loss = 0.64476627\n",
      "Iteration 83, loss = 0.64429175\n",
      "Iteration 84, loss = 0.64383591\n",
      "Iteration 85, loss = 0.64340044\n",
      "Iteration 86, loss = 0.64295312\n",
      "Iteration 87, loss = 0.64249595\n",
      "Iteration 88, loss = 0.64203889\n",
      "Iteration 89, loss = 0.64158786\n",
      "Iteration 90, loss = 0.64114646\n",
      "Iteration 91, loss = 0.64070076\n",
      "Iteration 92, loss = 0.64026881\n",
      "Iteration 93, loss = 0.63983340\n",
      "Iteration 94, loss = 0.63939625\n",
      "Iteration 95, loss = 0.63895025\n",
      "Iteration 96, loss = 0.63851056\n",
      "Iteration 97, loss = 0.63816182\n",
      "Iteration 98, loss = 0.63780223\n",
      "Iteration 99, loss = 0.63743574\n",
      "Iteration 100, loss = 0.63705013\n",
      "Iteration 101, loss = 0.63665555\n",
      "Iteration 102, loss = 0.63626928\n",
      "Iteration 103, loss = 0.63588662\n",
      "Iteration 104, loss = 0.63548699\n",
      "Iteration 105, loss = 0.63508238\n",
      "Iteration 106, loss = 0.63467447\n",
      "Iteration 107, loss = 0.63425655\n",
      "Iteration 108, loss = 0.63385533\n",
      "Iteration 109, loss = 0.63350195\n",
      "Iteration 110, loss = 0.63312779\n",
      "Iteration 111, loss = 0.63273243\n",
      "Iteration 112, loss = 0.63234027\n",
      "Iteration 113, loss = 0.63195304\n",
      "Iteration 114, loss = 0.63156679\n",
      "Iteration 115, loss = 0.63116636\n",
      "Iteration 116, loss = 0.63078180\n",
      "Iteration 117, loss = 0.63040559\n",
      "Iteration 118, loss = 0.63001791\n",
      "Iteration 119, loss = 0.62963506\n",
      "Iteration 120, loss = 0.62924659\n",
      "Iteration 121, loss = 0.62884384\n",
      "Iteration 122, loss = 0.62844793\n",
      "Iteration 123, loss = 0.62807613\n",
      "Iteration 124, loss = 0.62769125\n",
      "Iteration 125, loss = 0.62730184\n",
      "Iteration 126, loss = 0.62691940\n",
      "Iteration 127, loss = 0.62652858\n",
      "Iteration 128, loss = 0.62614246\n",
      "Iteration 129, loss = 0.62574390\n",
      "Iteration 130, loss = 0.62535987\n",
      "Iteration 131, loss = 0.62498724\n",
      "Iteration 132, loss = 0.62459080\n",
      "Iteration 133, loss = 0.62418776\n",
      "Iteration 134, loss = 0.62379662\n",
      "Iteration 135, loss = 0.62341397\n",
      "Iteration 136, loss = 0.62303190\n",
      "Iteration 137, loss = 0.62263368\n",
      "Iteration 138, loss = 0.62223519\n",
      "Iteration 139, loss = 0.62183871\n",
      "Iteration 140, loss = 0.62143636\n",
      "Iteration 141, loss = 0.62104568\n",
      "Iteration 142, loss = 0.62065396\n",
      "Iteration 143, loss = 0.62025330\n",
      "Iteration 144, loss = 0.61987089\n",
      "Iteration 145, loss = 0.61947825\n",
      "Iteration 146, loss = 0.61907250\n",
      "Iteration 147, loss = 0.61866907\n",
      "Iteration 148, loss = 0.61827036\n",
      "Iteration 149, loss = 0.61788449\n",
      "Iteration 150, loss = 0.61748662\n",
      "Iteration 151, loss = 0.61707312\n",
      "Iteration 152, loss = 0.61668460\n",
      "Iteration 153, loss = 0.61628809\n",
      "Iteration 154, loss = 0.61587723\n",
      "Iteration 155, loss = 0.61546699\n",
      "Iteration 156, loss = 0.61505767\n",
      "Iteration 157, loss = 0.61467050\n",
      "Iteration 158, loss = 0.61427135\n",
      "Iteration 159, loss = 0.61385950\n",
      "Iteration 160, loss = 0.61344936\n",
      "Iteration 161, loss = 0.61303008\n",
      "Iteration 162, loss = 0.61261926\n",
      "Iteration 163, loss = 0.61222428\n",
      "Iteration 164, loss = 0.61181146\n",
      "Iteration 165, loss = 0.61139559\n",
      "Iteration 166, loss = 0.61097478\n",
      "Iteration 167, loss = 0.61057277\n",
      "Iteration 168, loss = 0.61016474\n",
      "Iteration 169, loss = 0.60974064\n",
      "Iteration 170, loss = 0.60932575\n",
      "Iteration 171, loss = 0.60890971\n",
      "Iteration 172, loss = 0.60849139\n",
      "Iteration 173, loss = 0.60807003\n",
      "Iteration 174, loss = 0.60765925\n",
      "Iteration 175, loss = 0.60724402\n",
      "Iteration 176, loss = 0.60681628\n",
      "Iteration 177, loss = 0.60639089\n",
      "Iteration 178, loss = 0.60597543\n",
      "Iteration 179, loss = 0.60555071\n",
      "Iteration 180, loss = 0.60512957\n",
      "Iteration 181, loss = 0.60470886\n",
      "Iteration 182, loss = 0.60428653\n",
      "Iteration 183, loss = 0.60386285\n",
      "Iteration 184, loss = 0.60342630\n",
      "Iteration 185, loss = 0.60299417\n",
      "Iteration 186, loss = 0.60257188\n",
      "Iteration 187, loss = 0.60214344\n",
      "Iteration 188, loss = 0.60170688\n",
      "Iteration 189, loss = 0.60127411\n",
      "Iteration 190, loss = 0.60084175\n",
      "Iteration 191, loss = 0.60040221\n",
      "Iteration 192, loss = 0.59996473\n",
      "Iteration 193, loss = 0.59952872\n",
      "Iteration 194, loss = 0.59909968\n",
      "Iteration 195, loss = 0.59865698\n",
      "Iteration 196, loss = 0.59823226\n",
      "Iteration 197, loss = 0.59779011\n",
      "Iteration 198, loss = 0.59734967\n",
      "Iteration 199, loss = 0.59691567\n",
      "Iteration 200, loss = 0.59649141\n",
      "Iteration 201, loss = 0.59604522\n",
      "Iteration 202, loss = 0.59558801\n",
      "Iteration 203, loss = 0.59515236\n",
      "Iteration 204, loss = 0.59470903\n",
      "Iteration 205, loss = 0.59425621\n",
      "Iteration 206, loss = 0.59380726\n",
      "Iteration 207, loss = 0.59337654\n",
      "Iteration 208, loss = 0.59292687\n",
      "Iteration 209, loss = 0.59246636\n",
      "Iteration 210, loss = 0.59202019\n",
      "Iteration 211, loss = 0.59156638\n",
      "Iteration 212, loss = 0.59111535\n",
      "Iteration 213, loss = 0.59067042\n",
      "Iteration 214, loss = 0.59021717\n",
      "Iteration 215, loss = 0.58976277\n",
      "Iteration 216, loss = 0.58929672\n",
      "Iteration 217, loss = 0.58883990\n",
      "Iteration 218, loss = 0.58838372\n",
      "Iteration 219, loss = 0.58790807\n",
      "Iteration 220, loss = 0.58744989\n",
      "Iteration 221, loss = 0.58700677\n",
      "Iteration 222, loss = 0.58654849\n",
      "Iteration 223, loss = 0.58607845\n",
      "Iteration 224, loss = 0.58561374\n",
      "Iteration 225, loss = 0.58513636\n",
      "Iteration 226, loss = 0.58467752\n",
      "Iteration 227, loss = 0.58421884\n",
      "Iteration 228, loss = 0.58374441\n",
      "Iteration 229, loss = 0.58326624\n",
      "Iteration 230, loss = 0.58280333\n",
      "Iteration 231, loss = 0.58233699\n",
      "Iteration 232, loss = 0.58186815\n",
      "Iteration 233, loss = 0.58138810\n",
      "Iteration 234, loss = 0.58093437\n",
      "Iteration 235, loss = 0.58046076\n",
      "Iteration 236, loss = 0.57998375\n",
      "Iteration 237, loss = 0.57950348\n",
      "Iteration 238, loss = 0.57902375\n",
      "Iteration 239, loss = 0.57854389\n",
      "Iteration 240, loss = 0.57805823\n",
      "Iteration 241, loss = 0.57760180\n",
      "Iteration 242, loss = 0.57712458\n",
      "Iteration 243, loss = 0.57662980\n",
      "Iteration 244, loss = 0.57615366\n",
      "Iteration 245, loss = 0.57568305\n",
      "Iteration 246, loss = 0.57519855\n",
      "Iteration 247, loss = 0.57471300\n",
      "Iteration 248, loss = 0.57423468\n",
      "Iteration 249, loss = 0.57374247\n",
      "Iteration 250, loss = 0.57323425\n",
      "Iteration 251, loss = 0.57276469\n",
      "Iteration 252, loss = 0.57228877\n",
      "Iteration 253, loss = 0.57179124\n",
      "Iteration 254, loss = 0.57129743\n",
      "Iteration 255, loss = 0.57081091\n",
      "Iteration 256, loss = 0.57032122\n",
      "Iteration 257, loss = 0.56982314\n",
      "Iteration 258, loss = 0.56932821\n",
      "Iteration 259, loss = 0.56883079\n",
      "Iteration 260, loss = 0.56833988\n",
      "Iteration 261, loss = 0.56784005\n",
      "Iteration 262, loss = 0.56733576\n",
      "Iteration 263, loss = 0.56685326\n",
      "Iteration 264, loss = 0.56635089\n",
      "Iteration 265, loss = 0.56583651\n",
      "Iteration 266, loss = 0.56535493\n",
      "Iteration 267, loss = 0.56485650\n",
      "Iteration 268, loss = 0.56433923\n",
      "Iteration 269, loss = 0.56384872\n",
      "Iteration 270, loss = 0.56334382\n",
      "Iteration 271, loss = 0.56282579\n",
      "Iteration 272, loss = 0.56232252\n",
      "Iteration 273, loss = 0.56182508\n",
      "Iteration 274, loss = 0.56131574\n",
      "Iteration 275, loss = 0.56079220\n",
      "Iteration 276, loss = 0.56029486\n",
      "Iteration 277, loss = 0.55979055\n",
      "Iteration 278, loss = 0.55927392\n",
      "Iteration 279, loss = 0.55875166\n",
      "Iteration 280, loss = 0.55823707\n",
      "Iteration 281, loss = 0.55773404\n",
      "Iteration 282, loss = 0.55722107\n",
      "Iteration 283, loss = 0.55670006\n",
      "Iteration 284, loss = 0.55618078\n",
      "Iteration 285, loss = 0.55566893\n",
      "Iteration 286, loss = 0.55514833\n",
      "Iteration 287, loss = 0.55463126\n",
      "Iteration 288, loss = 0.55411778\n",
      "Iteration 289, loss = 0.55359833\n",
      "Iteration 290, loss = 0.55308151\n",
      "Iteration 291, loss = 0.55255513\n",
      "Iteration 292, loss = 0.55203260\n",
      "Iteration 293, loss = 0.55150947\n",
      "Iteration 294, loss = 0.55098013\n",
      "Iteration 295, loss = 0.55045234\n",
      "Iteration 296, loss = 0.54992760\n",
      "Iteration 297, loss = 0.54940206\n",
      "Iteration 298, loss = 0.54887954\n",
      "Iteration 299, loss = 0.54835253\n",
      "Iteration 300, loss = 0.54781759\n",
      "Iteration 301, loss = 0.54729569\n",
      "Iteration 302, loss = 0.54676772\n",
      "Iteration 303, loss = 0.54623446\n",
      "Iteration 304, loss = 0.54570853\n",
      "Iteration 305, loss = 0.54517294\n",
      "Iteration 306, loss = 0.54463354\n",
      "Iteration 307, loss = 0.54410049\n",
      "Iteration 308, loss = 0.54355967\n",
      "Iteration 309, loss = 0.54304027\n",
      "Iteration 310, loss = 0.54251067\n",
      "Iteration 311, loss = 0.54195713\n",
      "Iteration 312, loss = 0.54142326\n",
      "Iteration 313, loss = 0.54089085\n",
      "Iteration 314, loss = 0.54036046\n",
      "Iteration 315, loss = 0.53981680\n",
      "Iteration 316, loss = 0.53927837\n",
      "Iteration 317, loss = 0.53873142\n",
      "Iteration 318, loss = 0.53818268\n",
      "Iteration 319, loss = 0.53764198\n",
      "Iteration 320, loss = 0.53708966\n",
      "Iteration 321, loss = 0.53655064\n",
      "Iteration 322, loss = 0.53600907\n",
      "Iteration 323, loss = 0.53544892\n",
      "Iteration 324, loss = 0.53491026\n",
      "Iteration 325, loss = 0.53436593\n",
      "Iteration 326, loss = 0.53382996\n",
      "Iteration 327, loss = 0.53327801\n",
      "Iteration 328, loss = 0.53272511\n",
      "Iteration 329, loss = 0.53218158\n",
      "Iteration 330, loss = 0.53163886\n",
      "Iteration 331, loss = 0.53108158\n",
      "Iteration 332, loss = 0.53052499\n",
      "Iteration 333, loss = 0.52995148\n",
      "Iteration 334, loss = 0.52941058\n",
      "Iteration 335, loss = 0.52885951\n",
      "Iteration 336, loss = 0.52830331\n",
      "Iteration 337, loss = 0.52774165\n",
      "Iteration 338, loss = 0.52718434\n",
      "Iteration 339, loss = 0.52662444\n",
      "Iteration 340, loss = 0.52607187\n",
      "Iteration 341, loss = 0.52551133\n",
      "Iteration 342, loss = 0.52495089\n",
      "Iteration 343, loss = 0.52440831\n",
      "Iteration 344, loss = 0.52384767\n",
      "Iteration 345, loss = 0.52329629\n",
      "Iteration 346, loss = 0.52271399\n",
      "Iteration 347, loss = 0.52214463\n",
      "Iteration 348, loss = 0.52160626\n",
      "Iteration 349, loss = 0.52104310\n",
      "Iteration 350, loss = 0.52046871\n",
      "Iteration 351, loss = 0.51990771\n",
      "Iteration 352, loss = 0.51933565\n",
      "Iteration 353, loss = 0.51877215\n",
      "Iteration 354, loss = 0.51820780\n",
      "Iteration 355, loss = 0.51764091\n",
      "Iteration 356, loss = 0.51705860\n",
      "Iteration 357, loss = 0.51649508\n",
      "Iteration 358, loss = 0.51592609\n",
      "Iteration 359, loss = 0.51537690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 360, loss = 0.51480357\n",
      "Iteration 361, loss = 0.51422595\n",
      "Iteration 362, loss = 0.51365874\n",
      "Iteration 363, loss = 0.51308859\n",
      "Iteration 364, loss = 0.51250454\n",
      "Iteration 365, loss = 0.51196694\n",
      "Iteration 366, loss = 0.51140365\n",
      "Iteration 367, loss = 0.51079759\n",
      "Iteration 368, loss = 0.51024673\n",
      "Iteration 369, loss = 0.50968277\n",
      "Iteration 370, loss = 0.50912074\n",
      "Iteration 371, loss = 0.50855484\n",
      "Iteration 372, loss = 0.50798091\n",
      "Iteration 373, loss = 0.50741226\n",
      "Iteration 374, loss = 0.50683366\n",
      "Iteration 375, loss = 0.50626577\n",
      "Iteration 376, loss = 0.50569049\n",
      "Iteration 377, loss = 0.50511843\n",
      "Iteration 378, loss = 0.50453615\n",
      "Iteration 379, loss = 0.50398759\n",
      "Iteration 380, loss = 0.50339032\n",
      "Iteration 381, loss = 0.50281828\n",
      "Iteration 382, loss = 0.50224721\n",
      "Iteration 383, loss = 0.50167866\n",
      "Iteration 384, loss = 0.50111339\n",
      "Iteration 385, loss = 0.50052080\n",
      "Iteration 386, loss = 0.49996708\n",
      "Iteration 387, loss = 0.49939305\n",
      "Iteration 388, loss = 0.49881144\n",
      "Iteration 389, loss = 0.49823247\n",
      "Iteration 390, loss = 0.49766145\n",
      "Iteration 391, loss = 0.49707518\n",
      "Iteration 392, loss = 0.49649597\n",
      "Iteration 393, loss = 0.49595506\n",
      "Iteration 394, loss = 0.49536838\n",
      "Iteration 395, loss = 0.49478436\n",
      "Iteration 396, loss = 0.49418601\n",
      "Iteration 397, loss = 0.49362851\n",
      "Iteration 398, loss = 0.49305158\n",
      "Iteration 399, loss = 0.49246903\n",
      "Iteration 400, loss = 0.49187808\n",
      "Iteration 401, loss = 0.49132407\n",
      "Iteration 402, loss = 0.49073347\n",
      "Iteration 403, loss = 0.49015027\n",
      "Iteration 404, loss = 0.48955827\n",
      "Iteration 405, loss = 0.48899586\n",
      "Iteration 406, loss = 0.48840155\n",
      "Iteration 407, loss = 0.48781980\n",
      "Iteration 408, loss = 0.48723899\n",
      "Iteration 409, loss = 0.48664701\n",
      "Iteration 410, loss = 0.48606240\n",
      "Iteration 411, loss = 0.48548269\n",
      "Iteration 412, loss = 0.48489798\n",
      "Iteration 413, loss = 0.48430604\n",
      "Iteration 414, loss = 0.48373749\n",
      "Iteration 415, loss = 0.48314112\n",
      "Iteration 416, loss = 0.48254934\n",
      "Iteration 417, loss = 0.48197492\n",
      "Iteration 418, loss = 0.48137645\n",
      "Iteration 419, loss = 0.48079103\n",
      "Iteration 420, loss = 0.48019670\n",
      "Iteration 421, loss = 0.47962324\n",
      "Iteration 422, loss = 0.47903863\n",
      "Iteration 423, loss = 0.47844062\n",
      "Iteration 424, loss = 0.47785767\n",
      "Iteration 425, loss = 0.47727666\n",
      "Iteration 426, loss = 0.47668420\n",
      "Iteration 427, loss = 0.47609094\n",
      "Iteration 428, loss = 0.47550399\n",
      "Iteration 429, loss = 0.47494397\n",
      "Iteration 430, loss = 0.47439603\n",
      "Iteration 431, loss = 0.47386379\n",
      "Iteration 432, loss = 0.47333187\n",
      "Iteration 433, loss = 0.47280436\n",
      "Iteration 434, loss = 0.47224827\n",
      "Iteration 435, loss = 0.47169269\n",
      "Iteration 436, loss = 0.47115392\n",
      "Iteration 437, loss = 0.47059619\n",
      "Iteration 438, loss = 0.47001538\n",
      "Iteration 439, loss = 0.46945665\n",
      "Iteration 440, loss = 0.46889534\n",
      "Iteration 441, loss = 0.46832889\n",
      "Iteration 442, loss = 0.46773753\n",
      "Iteration 443, loss = 0.46716972\n",
      "Iteration 444, loss = 0.46658862\n",
      "Iteration 445, loss = 0.46603224\n",
      "Iteration 446, loss = 0.46547572\n",
      "Iteration 447, loss = 0.46492833\n",
      "Iteration 448, loss = 0.46437995\n",
      "Iteration 449, loss = 0.46382537\n",
      "Iteration 450, loss = 0.46326676\n",
      "Iteration 451, loss = 0.46272064\n",
      "Iteration 452, loss = 0.46215189\n",
      "Iteration 453, loss = 0.46160017\n",
      "Iteration 454, loss = 0.46103001\n",
      "Iteration 455, loss = 0.46043714\n",
      "Iteration 456, loss = 0.45990424\n",
      "Iteration 457, loss = 0.45934539\n",
      "Iteration 458, loss = 0.45877844\n",
      "Iteration 459, loss = 0.45820904\n",
      "Iteration 460, loss = 0.45766639\n",
      "Iteration 461, loss = 0.45709191\n",
      "Iteration 462, loss = 0.45652842\n",
      "Iteration 463, loss = 0.45601662\n",
      "Iteration 464, loss = 0.45544094\n",
      "Iteration 465, loss = 0.45485778\n",
      "Iteration 466, loss = 0.45430757\n",
      "Iteration 467, loss = 0.45374780\n",
      "Iteration 468, loss = 0.45320091\n",
      "Iteration 469, loss = 0.45264050\n",
      "Iteration 470, loss = 0.45208596\n",
      "Iteration 471, loss = 0.45153701\n",
      "Iteration 472, loss = 0.45097262\n",
      "Iteration 473, loss = 0.45038415\n",
      "Iteration 474, loss = 0.44981232\n",
      "Iteration 475, loss = 0.44926899\n",
      "Iteration 476, loss = 0.44869928\n",
      "Iteration 477, loss = 0.44811380\n",
      "Iteration 478, loss = 0.44757278\n",
      "Iteration 479, loss = 0.44701490\n",
      "Iteration 480, loss = 0.44645136\n",
      "Iteration 481, loss = 0.44591955\n",
      "Iteration 482, loss = 0.44536036\n",
      "Iteration 483, loss = 0.44477907\n",
      "Iteration 484, loss = 0.44420052\n",
      "Iteration 485, loss = 0.44367073\n",
      "Iteration 486, loss = 0.44309490\n",
      "Iteration 487, loss = 0.44252203\n",
      "Iteration 488, loss = 0.44197721\n",
      "Iteration 489, loss = 0.44142791\n",
      "Iteration 490, loss = 0.44089808\n",
      "Iteration 491, loss = 0.44047163\n",
      "Iteration 492, loss = 0.43992222\n",
      "Iteration 493, loss = 0.43931138\n",
      "Iteration 494, loss = 0.43881335\n",
      "Iteration 495, loss = 0.43828773\n",
      "Iteration 496, loss = 0.43770327\n",
      "Iteration 497, loss = 0.43716970\n",
      "Iteration 498, loss = 0.43669803\n",
      "Iteration 499, loss = 0.43617331\n",
      "Iteration 500, loss = 0.43560522\n",
      "Iteration 501, loss = 0.43505501\n",
      "Iteration 502, loss = 0.43455178\n",
      "Iteration 503, loss = 0.43401133\n",
      "Iteration 504, loss = 0.43342294\n",
      "Iteration 505, loss = 0.43288721\n",
      "Iteration 506, loss = 0.43236795\n",
      "Iteration 507, loss = 0.43181572\n",
      "Iteration 508, loss = 0.43123817\n",
      "Iteration 509, loss = 0.43069566\n",
      "Iteration 510, loss = 0.43015295\n",
      "Iteration 511, loss = 0.42959964\n",
      "Iteration 512, loss = 0.42908025\n",
      "Iteration 513, loss = 0.42855398\n",
      "Iteration 514, loss = 0.42802012\n",
      "Iteration 515, loss = 0.42748407\n",
      "Iteration 516, loss = 0.42692585\n",
      "Iteration 517, loss = 0.42641407\n",
      "Iteration 518, loss = 0.42586093\n",
      "Iteration 519, loss = 0.42534476\n",
      "Iteration 520, loss = 0.42483912\n",
      "Iteration 521, loss = 0.42431204\n",
      "Iteration 522, loss = 0.42375032\n",
      "Iteration 523, loss = 0.42323805\n",
      "Iteration 524, loss = 0.42271151\n",
      "Iteration 525, loss = 0.42217079\n",
      "Iteration 526, loss = 0.42161178\n",
      "Iteration 527, loss = 0.42106357\n",
      "Iteration 528, loss = 0.42055412\n",
      "Iteration 529, loss = 0.42001164\n",
      "Iteration 530, loss = 0.41945802\n",
      "Iteration 531, loss = 0.41896206\n",
      "Iteration 532, loss = 0.41843377\n",
      "Iteration 533, loss = 0.41790000\n",
      "Iteration 534, loss = 0.41737514\n",
      "Iteration 535, loss = 0.41683435\n",
      "Iteration 536, loss = 0.41627554\n",
      "Iteration 537, loss = 0.41575417\n",
      "Iteration 538, loss = 0.41521245\n",
      "Iteration 539, loss = 0.41468842\n",
      "Iteration 540, loss = 0.41416527\n",
      "Iteration 541, loss = 0.41361275\n",
      "Iteration 542, loss = 0.41308636\n",
      "Iteration 543, loss = 0.41257605\n",
      "Iteration 544, loss = 0.41203879\n",
      "Iteration 545, loss = 0.41152969\n",
      "Iteration 546, loss = 0.41098191\n",
      "Iteration 547, loss = 0.41043596\n",
      "Iteration 548, loss = 0.40991871\n",
      "Iteration 549, loss = 0.40937098\n",
      "Iteration 550, loss = 0.40883466\n",
      "Iteration 551, loss = 0.40835509\n",
      "Iteration 552, loss = 0.40780410\n",
      "Iteration 553, loss = 0.40728214\n",
      "Iteration 554, loss = 0.40676342\n",
      "Iteration 555, loss = 0.40626023\n",
      "Iteration 556, loss = 0.40572531\n",
      "Iteration 557, loss = 0.40517899\n",
      "Iteration 558, loss = 0.40467867\n",
      "Iteration 559, loss = 0.40415488\n",
      "Iteration 560, loss = 0.40360719\n",
      "Iteration 561, loss = 0.40306257\n",
      "Iteration 562, loss = 0.40251632\n",
      "Iteration 563, loss = 0.40207446\n",
      "Iteration 564, loss = 0.40154488\n",
      "Iteration 565, loss = 0.40096007\n",
      "Iteration 566, loss = 0.40044035\n",
      "Iteration 567, loss = 0.39992997\n",
      "Iteration 568, loss = 0.39938186\n",
      "Iteration 569, loss = 0.39884479\n",
      "Iteration 570, loss = 0.39835161\n",
      "Iteration 571, loss = 0.39782555\n",
      "Iteration 572, loss = 0.39726565\n",
      "Iteration 573, loss = 0.39676485\n",
      "Iteration 574, loss = 0.39626182\n",
      "Iteration 575, loss = 0.39573493\n",
      "Iteration 576, loss = 0.39519728\n",
      "Iteration 577, loss = 0.39466536\n",
      "Iteration 578, loss = 0.39413045\n",
      "Iteration 579, loss = 0.39361404\n",
      "Iteration 580, loss = 0.39310201\n",
      "Iteration 581, loss = 0.39259400\n",
      "Iteration 582, loss = 0.39204692\n",
      "Iteration 583, loss = 0.39149420\n",
      "Iteration 584, loss = 0.39097936\n",
      "Iteration 585, loss = 0.39043969\n",
      "Iteration 586, loss = 0.38995636\n",
      "Iteration 587, loss = 0.38942768\n",
      "Iteration 588, loss = 0.38890237\n",
      "Iteration 589, loss = 0.38839060\n",
      "Iteration 590, loss = 0.38785317\n",
      "Iteration 591, loss = 0.38731091\n",
      "Iteration 592, loss = 0.38678344\n",
      "Iteration 593, loss = 0.38628047\n",
      "Iteration 594, loss = 0.38576312\n",
      "Iteration 595, loss = 0.38525552\n",
      "Iteration 596, loss = 0.38473586\n",
      "Iteration 597, loss = 0.38419761\n",
      "Iteration 598, loss = 0.38368961\n",
      "Iteration 599, loss = 0.38316992\n",
      "Iteration 600, loss = 0.38262880\n",
      "Iteration 601, loss = 0.38211046\n",
      "Iteration 602, loss = 0.38159634\n",
      "Iteration 603, loss = 0.38109745\n",
      "Iteration 604, loss = 0.38057614\n",
      "Iteration 605, loss = 0.38003193\n",
      "Iteration 606, loss = 0.37952983\n",
      "Iteration 607, loss = 0.37901345\n",
      "Iteration 608, loss = 0.37852192\n",
      "Iteration 609, loss = 0.37799803\n",
      "Iteration 610, loss = 0.37749912\n",
      "Iteration 611, loss = 0.37697606\n",
      "Iteration 612, loss = 0.37646057\n",
      "Iteration 613, loss = 0.37592862\n",
      "Iteration 614, loss = 0.37541807\n",
      "Iteration 615, loss = 0.37490824\n",
      "Iteration 616, loss = 0.37441061\n",
      "Iteration 617, loss = 0.37389234\n",
      "Iteration 618, loss = 0.37337778\n",
      "Iteration 619, loss = 0.37286212\n",
      "Iteration 620, loss = 0.37234400\n",
      "Iteration 621, loss = 0.37181057\n",
      "Iteration 622, loss = 0.37128609\n",
      "Iteration 623, loss = 0.37081137\n",
      "Iteration 624, loss = 0.37030150\n",
      "Iteration 625, loss = 0.36976204\n",
      "Iteration 626, loss = 0.36922916\n",
      "Iteration 627, loss = 0.36876660\n",
      "Iteration 628, loss = 0.36826453\n",
      "Iteration 629, loss = 0.36771855\n",
      "Iteration 630, loss = 0.36726107\n",
      "Iteration 631, loss = 0.36676143\n",
      "Iteration 632, loss = 0.36623316\n",
      "Iteration 633, loss = 0.36568774\n",
      "Iteration 634, loss = 0.36517838\n",
      "Iteration 635, loss = 0.36467196\n",
      "Iteration 636, loss = 0.36418748\n",
      "Iteration 637, loss = 0.36367393\n",
      "Iteration 638, loss = 0.36314748\n",
      "Iteration 639, loss = 0.36264922\n",
      "Iteration 640, loss = 0.36217185\n",
      "Iteration 641, loss = 0.36168579\n",
      "Iteration 642, loss = 0.36116463\n",
      "Iteration 643, loss = 0.36061410\n",
      "Iteration 644, loss = 0.36011867\n",
      "Iteration 645, loss = 0.35965816\n",
      "Iteration 646, loss = 0.35912745\n",
      "Iteration 647, loss = 0.35859315\n",
      "Iteration 648, loss = 0.35810014\n",
      "Iteration 649, loss = 0.35758778\n",
      "Iteration 650, loss = 0.35704659\n",
      "Iteration 651, loss = 0.35658784\n",
      "Iteration 652, loss = 0.35609002\n",
      "Iteration 653, loss = 0.35557035\n",
      "Iteration 654, loss = 0.35508397\n",
      "Iteration 655, loss = 0.35456596\n",
      "Iteration 656, loss = 0.35405876\n",
      "Iteration 657, loss = 0.35353112\n",
      "Iteration 658, loss = 0.35302133\n",
      "Iteration 659, loss = 0.35252794\n",
      "Iteration 660, loss = 0.35203033\n",
      "Iteration 661, loss = 0.35151061\n",
      "Iteration 662, loss = 0.35101754\n",
      "Iteration 663, loss = 0.35052925\n",
      "Iteration 664, loss = 0.35002661\n",
      "Iteration 665, loss = 0.34953081\n",
      "Iteration 666, loss = 0.34902211\n",
      "Iteration 667, loss = 0.34849813\n",
      "Iteration 668, loss = 0.34802239\n",
      "Iteration 669, loss = 0.34752866\n",
      "Iteration 670, loss = 0.34698117\n",
      "Iteration 671, loss = 0.34649588\n",
      "Iteration 672, loss = 0.34602289\n",
      "Iteration 673, loss = 0.34551878\n",
      "Iteration 674, loss = 0.34501317\n",
      "Iteration 675, loss = 0.34450535\n",
      "Iteration 676, loss = 0.34400569\n",
      "Iteration 677, loss = 0.34352170\n",
      "Iteration 678, loss = 0.34302725\n",
      "Iteration 679, loss = 0.34249941\n",
      "Iteration 680, loss = 0.34201902\n",
      "Iteration 681, loss = 0.34152140\n",
      "Iteration 682, loss = 0.34104502\n",
      "Iteration 683, loss = 0.34055778\n",
      "Iteration 684, loss = 0.34003830\n",
      "Iteration 685, loss = 0.33951227\n",
      "Iteration 686, loss = 0.33902750\n",
      "Iteration 687, loss = 0.33855636\n",
      "Iteration 688, loss = 0.33806749\n",
      "Iteration 689, loss = 0.33755916\n",
      "Iteration 690, loss = 0.33707124\n",
      "Iteration 691, loss = 0.33655291\n",
      "Iteration 692, loss = 0.33611162\n",
      "Iteration 693, loss = 0.33562247\n",
      "Iteration 694, loss = 0.33513757\n",
      "Iteration 695, loss = 0.33462684\n",
      "Iteration 696, loss = 0.33414608\n",
      "Iteration 697, loss = 0.33363390\n",
      "Iteration 698, loss = 0.33317239\n",
      "Iteration 699, loss = 0.33269795\n",
      "Iteration 700, loss = 0.33221447\n",
      "Iteration 701, loss = 0.33168382\n",
      "Iteration 702, loss = 0.33118176\n",
      "Iteration 703, loss = 0.33072854\n",
      "Iteration 704, loss = 0.33023337\n",
      "Iteration 705, loss = 0.32973874\n",
      "Iteration 706, loss = 0.32923468\n",
      "Iteration 707, loss = 0.32875424\n",
      "Iteration 708, loss = 0.32827409\n",
      "Iteration 709, loss = 0.32775845\n",
      "Iteration 710, loss = 0.32730607\n",
      "Iteration 711, loss = 0.32684427\n",
      "Iteration 712, loss = 0.32635490\n",
      "Iteration 713, loss = 0.32586108\n",
      "Iteration 714, loss = 0.32537117\n",
      "Iteration 715, loss = 0.32489430\n",
      "Iteration 716, loss = 0.32441168\n",
      "Iteration 717, loss = 0.32394004\n",
      "Iteration 718, loss = 0.32345965\n",
      "Iteration 719, loss = 0.32299692\n",
      "Iteration 720, loss = 0.32250909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 721, loss = 0.32203816\n",
      "Iteration 722, loss = 0.32156461\n",
      "Iteration 723, loss = 0.32105903\n",
      "Iteration 724, loss = 0.32061133\n",
      "Iteration 725, loss = 0.32014723\n",
      "Iteration 726, loss = 0.31966316\n",
      "Iteration 727, loss = 0.31919831\n",
      "Iteration 728, loss = 0.31873555\n",
      "Iteration 729, loss = 0.31826763\n",
      "Iteration 730, loss = 0.31776331\n",
      "Iteration 731, loss = 0.31726536\n",
      "Iteration 732, loss = 0.31680543\n",
      "Iteration 733, loss = 0.31635364\n",
      "Iteration 734, loss = 0.31586197\n",
      "Iteration 735, loss = 0.31538564\n",
      "Iteration 736, loss = 0.31493294\n",
      "Iteration 737, loss = 0.31445458\n",
      "Iteration 738, loss = 0.31396567\n",
      "Iteration 739, loss = 0.31347869\n",
      "Iteration 740, loss = 0.31303598\n",
      "Iteration 741, loss = 0.31256413\n",
      "Iteration 742, loss = 0.31210087\n",
      "Iteration 743, loss = 0.31164159\n",
      "Iteration 744, loss = 0.31117576\n",
      "Iteration 745, loss = 0.31069368\n",
      "Iteration 746, loss = 0.31020468\n",
      "Iteration 747, loss = 0.30974617\n",
      "Iteration 748, loss = 0.30929900\n",
      "Iteration 749, loss = 0.30882101\n",
      "Iteration 750, loss = 0.30835240\n",
      "Iteration 751, loss = 0.30788507\n",
      "Iteration 752, loss = 0.30741979\n",
      "Iteration 753, loss = 0.30695017\n",
      "Iteration 754, loss = 0.30648580\n",
      "Iteration 755, loss = 0.30599911\n",
      "Iteration 756, loss = 0.30555818\n",
      "Iteration 757, loss = 0.30510675\n",
      "Iteration 758, loss = 0.30463783\n",
      "Iteration 759, loss = 0.30417210\n",
      "Iteration 760, loss = 0.30371831\n",
      "Iteration 761, loss = 0.30324841\n",
      "Iteration 762, loss = 0.30277885\n",
      "Iteration 763, loss = 0.30232057\n",
      "Iteration 764, loss = 0.30186321\n",
      "Iteration 765, loss = 0.30141519\n",
      "Iteration 766, loss = 0.30097678\n",
      "Iteration 767, loss = 0.30051449\n",
      "Iteration 768, loss = 0.30006256\n",
      "Iteration 769, loss = 0.29960572\n",
      "Iteration 770, loss = 0.29913382\n",
      "Iteration 771, loss = 0.29869419\n",
      "Iteration 772, loss = 0.29823386\n",
      "Iteration 773, loss = 0.29778567\n",
      "Iteration 774, loss = 0.29733948\n",
      "Iteration 775, loss = 0.29688483\n",
      "Iteration 776, loss = 0.29641993\n",
      "Iteration 777, loss = 0.29598927\n",
      "Iteration 778, loss = 0.29552944\n",
      "Iteration 779, loss = 0.29506378\n",
      "Iteration 780, loss = 0.29461871\n",
      "Iteration 781, loss = 0.29415064\n",
      "Iteration 782, loss = 0.29369043\n",
      "Iteration 783, loss = 0.29323140\n",
      "Iteration 784, loss = 0.29279327\n",
      "Iteration 785, loss = 0.29234132\n",
      "Iteration 786, loss = 0.29188193\n",
      "Iteration 787, loss = 0.29143221\n",
      "Iteration 788, loss = 0.29096927\n",
      "Iteration 789, loss = 0.29053914\n",
      "Iteration 790, loss = 0.29009341\n",
      "Iteration 791, loss = 0.28964366\n",
      "Iteration 792, loss = 0.28919396\n",
      "Iteration 793, loss = 0.28873704\n",
      "Iteration 794, loss = 0.28830902\n",
      "Iteration 795, loss = 0.28790111\n",
      "Iteration 796, loss = 0.28745078\n",
      "Iteration 797, loss = 0.28698584\n",
      "Iteration 798, loss = 0.28655283\n",
      "Iteration 799, loss = 0.28610366\n",
      "Iteration 800, loss = 0.28565257\n",
      "Iteration 801, loss = 0.28520168\n",
      "Iteration 802, loss = 0.28474808\n",
      "Iteration 803, loss = 0.28434759\n",
      "Iteration 804, loss = 0.28391321\n",
      "Iteration 805, loss = 0.28343005\n",
      "Iteration 806, loss = 0.28301930\n",
      "Iteration 807, loss = 0.28259758\n",
      "Iteration 808, loss = 0.28214512\n",
      "Iteration 809, loss = 0.28172489\n",
      "Iteration 810, loss = 0.28128512\n",
      "Iteration 811, loss = 0.28082884\n",
      "Iteration 812, loss = 0.28040125\n",
      "Iteration 813, loss = 0.27997477\n",
      "Iteration 814, loss = 0.27954034\n",
      "Iteration 815, loss = 0.27908109\n",
      "Iteration 816, loss = 0.27863071\n",
      "Iteration 817, loss = 0.27820566\n",
      "Iteration 818, loss = 0.27781199\n",
      "Iteration 819, loss = 0.27737495\n",
      "Iteration 820, loss = 0.27690677\n",
      "Iteration 821, loss = 0.27646856\n",
      "Iteration 822, loss = 0.27605009\n",
      "Iteration 823, loss = 0.27565241\n",
      "Iteration 824, loss = 0.27523381\n",
      "Iteration 825, loss = 0.27479481\n",
      "Iteration 826, loss = 0.27433183\n",
      "Iteration 827, loss = 0.27391641\n",
      "Iteration 828, loss = 0.27350194\n",
      "Iteration 829, loss = 0.27305060\n",
      "Iteration 830, loss = 0.27260189\n",
      "Iteration 831, loss = 0.27219976\n",
      "Iteration 832, loss = 0.27175258\n",
      "Iteration 833, loss = 0.27131833\n",
      "Iteration 834, loss = 0.27091813\n",
      "Iteration 835, loss = 0.27047727\n",
      "Iteration 836, loss = 0.27005125\n",
      "Iteration 837, loss = 0.26963211\n",
      "Iteration 838, loss = 0.26919360\n",
      "Iteration 839, loss = 0.26877506\n",
      "Iteration 840, loss = 0.26835687\n",
      "Iteration 841, loss = 0.26794522\n",
      "Iteration 842, loss = 0.26753259\n",
      "Iteration 843, loss = 0.26710079\n",
      "Iteration 844, loss = 0.26666468\n",
      "Iteration 845, loss = 0.26625386\n",
      "Iteration 846, loss = 0.26583853\n",
      "Iteration 847, loss = 0.26541104\n",
      "Iteration 848, loss = 0.26498328\n",
      "Iteration 849, loss = 0.26458977\n",
      "Iteration 850, loss = 0.26416829\n",
      "Iteration 851, loss = 0.26374058\n",
      "Iteration 852, loss = 0.26332944\n",
      "Iteration 853, loss = 0.26289814\n",
      "Iteration 854, loss = 0.26250707\n",
      "Iteration 855, loss = 0.26211114\n",
      "Iteration 856, loss = 0.26170233\n",
      "Iteration 857, loss = 0.26127660\n",
      "Iteration 858, loss = 0.26086060\n",
      "Iteration 859, loss = 0.26044032\n",
      "Iteration 860, loss = 0.26002182\n",
      "Iteration 861, loss = 0.25963841\n",
      "Iteration 862, loss = 0.25924795\n",
      "Iteration 863, loss = 0.25880123\n",
      "Iteration 864, loss = 0.25839422\n",
      "Iteration 865, loss = 0.25801133\n",
      "Iteration 866, loss = 0.25759092\n",
      "Iteration 867, loss = 0.25717869\n",
      "Iteration 868, loss = 0.25680635\n",
      "Iteration 869, loss = 0.25638059\n",
      "Iteration 870, loss = 0.25596580\n",
      "Iteration 871, loss = 0.25556545\n",
      "Iteration 872, loss = 0.25516333\n",
      "Iteration 873, loss = 0.25474833\n",
      "Iteration 874, loss = 0.25432487\n",
      "Iteration 875, loss = 0.25389500\n",
      "Iteration 876, loss = 0.25352613\n",
      "Iteration 877, loss = 0.25315166\n",
      "Iteration 878, loss = 0.25272508\n",
      "Iteration 879, loss = 0.25228846\n",
      "Iteration 880, loss = 0.25189642\n",
      "Iteration 881, loss = 0.25151589\n",
      "Iteration 882, loss = 0.25110764\n",
      "Iteration 883, loss = 0.25070005\n",
      "Iteration 884, loss = 0.25030681\n",
      "Iteration 885, loss = 0.24989672\n",
      "Iteration 886, loss = 0.24952210\n",
      "Iteration 887, loss = 0.24911312\n",
      "Iteration 888, loss = 0.24869917\n",
      "Iteration 889, loss = 0.24830898\n",
      "Iteration 890, loss = 0.24792614\n",
      "Iteration 891, loss = 0.24752742\n",
      "Iteration 892, loss = 0.24712935\n",
      "Iteration 893, loss = 0.24673470\n",
      "Iteration 894, loss = 0.24633391\n",
      "Iteration 895, loss = 0.24594063\n",
      "Iteration 896, loss = 0.24556658\n",
      "Iteration 897, loss = 0.24515775\n",
      "Iteration 898, loss = 0.24473118\n",
      "Iteration 899, loss = 0.24438011\n",
      "Iteration 900, loss = 0.24399596\n",
      "Iteration 901, loss = 0.24357828\n",
      "Iteration 902, loss = 0.24320393\n",
      "Iteration 903, loss = 0.24281978\n",
      "Iteration 904, loss = 0.24240869\n",
      "Iteration 905, loss = 0.24201372\n",
      "Iteration 906, loss = 0.24164256\n",
      "Iteration 907, loss = 0.24126735\n",
      "Iteration 908, loss = 0.24087507\n",
      "Iteration 909, loss = 0.24049329\n",
      "Iteration 910, loss = 0.24010569\n",
      "Iteration 911, loss = 0.23971132\n",
      "Iteration 912, loss = 0.23931526\n",
      "Iteration 913, loss = 0.23893105\n",
      "Iteration 914, loss = 0.23854906\n",
      "Iteration 915, loss = 0.23816624\n",
      "Iteration 916, loss = 0.23779434\n",
      "Iteration 917, loss = 0.23740925\n",
      "Iteration 918, loss = 0.23702724\n",
      "Iteration 919, loss = 0.23666557\n",
      "Iteration 920, loss = 0.23627107\n",
      "Iteration 921, loss = 0.23588674\n",
      "Iteration 922, loss = 0.23552553\n",
      "Iteration 923, loss = 0.23513865\n",
      "Iteration 924, loss = 0.23475556\n",
      "Iteration 925, loss = 0.23437995\n",
      "Iteration 926, loss = 0.23399881\n",
      "Iteration 927, loss = 0.23362341\n",
      "Iteration 928, loss = 0.23325052\n",
      "Iteration 929, loss = 0.23286910\n",
      "Iteration 930, loss = 0.23248190\n",
      "Iteration 931, loss = 0.23213065\n",
      "Iteration 932, loss = 0.23176449\n",
      "Iteration 933, loss = 0.23137648\n",
      "Iteration 934, loss = 0.23099559\n",
      "Iteration 935, loss = 0.23061383\n",
      "Iteration 936, loss = 0.23025425\n",
      "Iteration 937, loss = 0.22988058\n",
      "Iteration 938, loss = 0.22952551\n",
      "Iteration 939, loss = 0.22915707\n",
      "Iteration 940, loss = 0.22877241\n",
      "Iteration 941, loss = 0.22841846\n",
      "Iteration 942, loss = 0.22805429\n",
      "Iteration 943, loss = 0.22769059\n",
      "Iteration 944, loss = 0.22731667\n",
      "Iteration 945, loss = 0.22692528\n",
      "Iteration 946, loss = 0.22656491\n",
      "Iteration 947, loss = 0.22621759\n",
      "Iteration 948, loss = 0.22585387\n",
      "Iteration 949, loss = 0.22547396\n",
      "Iteration 950, loss = 0.22510682\n",
      "Iteration 951, loss = 0.22475757\n",
      "Iteration 952, loss = 0.22439695\n",
      "Iteration 953, loss = 0.22400462\n",
      "Iteration 954, loss = 0.22366241\n",
      "Iteration 955, loss = 0.22330728\n",
      "Iteration 956, loss = 0.22293632\n",
      "Iteration 957, loss = 0.22258073\n",
      "Iteration 958, loss = 0.22222600\n",
      "Iteration 959, loss = 0.22185671\n",
      "Iteration 960, loss = 0.22150349\n",
      "Iteration 961, loss = 0.22113873\n",
      "Iteration 962, loss = 0.22077682\n",
      "Iteration 963, loss = 0.22041658\n",
      "Iteration 964, loss = 0.22007256\n",
      "Iteration 965, loss = 0.21972568\n",
      "Iteration 966, loss = 0.21936629\n",
      "Iteration 967, loss = 0.21899444\n",
      "Iteration 968, loss = 0.21863374\n",
      "Iteration 969, loss = 0.21830897\n",
      "Iteration 970, loss = 0.21795442\n",
      "Iteration 971, loss = 0.21757296\n",
      "Iteration 972, loss = 0.21723201\n",
      "Iteration 973, loss = 0.21691804\n",
      "Iteration 974, loss = 0.21653568\n",
      "Iteration 975, loss = 0.21618124\n",
      "Iteration 976, loss = 0.21584235\n",
      "Iteration 977, loss = 0.21549985\n",
      "Iteration 978, loss = 0.21514925\n",
      "Iteration 979, loss = 0.21478827\n",
      "Iteration 980, loss = 0.21443742\n",
      "Iteration 981, loss = 0.21409122\n",
      "Iteration 982, loss = 0.21374640\n",
      "Iteration 983, loss = 0.21340898\n",
      "Iteration 984, loss = 0.21304545\n",
      "Iteration 985, loss = 0.21270897\n",
      "Iteration 986, loss = 0.21235752\n",
      "Iteration 987, loss = 0.21202266\n",
      "Iteration 988, loss = 0.21167811\n",
      "Iteration 989, loss = 0.21132576\n",
      "Iteration 990, loss = 0.21097352\n",
      "Iteration 991, loss = 0.21065443\n",
      "Iteration 992, loss = 0.21032270\n",
      "Iteration 993, loss = 0.20996249\n",
      "Iteration 994, loss = 0.20960710\n",
      "Iteration 995, loss = 0.20927492\n",
      "Iteration 996, loss = 0.20894497\n",
      "Iteration 997, loss = 0.20859249\n",
      "Iteration 998, loss = 0.20827209\n",
      "Iteration 999, loss = 0.20794487\n",
      "Iteration 1000, loss = 0.20760969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(10,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = mlpc.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
