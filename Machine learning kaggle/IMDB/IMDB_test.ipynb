{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pycharm-a175d104",
   "display_name": "PyCharm (mine)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "d:\\OneDrive - zju.edu.cn\\Documents\\Git\\Machine-learning\\Machine learning kaggle\\IMDB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# print(os.getcwd())\n",
    "os.chdir('D:\\OneDrive - zju.edu.cn\\Documents\\Git\\Machine-learning\\Machine learning kaggle\\IMDB')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "       id  sentiment                                             review\n0  5814_8          1  With all this stuff going down at the moment w...\n1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n3  3630_4          0  It must be assumed that those who praised this...\n4  9495_8          1  Superbly trashy and wondrously unpretentious 8...\n         id                                             review\n0  12311_10  Naturally in a film who's main themes are of m...\n1    8348_2  This movie is a disaster within a disaster fil...\n2    5828_4  All in all, this is a movie for kids. We saw i...\n3    7186_2  Afraid of the Dark left me with the impression...\n4   12128_7  A very accurate depiction of small time mob li...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('datasets/labeledTrainData.tsv',delimiter='\\t')\n",
    "test_data = pd.read_csv('datasets/testData.tsv',delimiter='\\t')\n",
    "print(train_data.head())\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review = train_data.loc[0,'review']\n",
    "# print('review:\\n',review)\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# def review_to_text(review,remove_stopwords):\n",
    "#     raw_text = BeautifulSoup(review,'html').get_text()\n",
    "#     letters = re.sub('[^a-zA-Z]',' ',raw_text)\n",
    "#     words = letters.lower().split()\n",
    "#     print('words:\\n',words)\n",
    "#     if remove_stopwords: \n",
    "#         words = [w for w in words if w not in stop_words]\n",
    "#     print('words:\\n',words)\n",
    "#     return words\n",
    "\n",
    "# x_train = []\n",
    "# x_train.append(' '.join(review_to_text(review,True)))\n",
    "# print('x_train:\\n',x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def review_to_text(review,remove_stopwords):\n",
    "    raw_text = BeautifulSoup(review,'html').get_text()\n",
    "    letters = re.sub('[^a-zA-Z]',' ',raw_text)\n",
    "    words = letters.lower().split()\n",
    "    if remove_stopwords: \n",
    "        words = [w for w in words if w not in stop_words]\n",
    "    return words\n",
    "\n",
    "x_train = []\n",
    "for review in train_data['review']:\n",
    "    x_train.append(' '.join(review_to_text(review,True)))\n",
    "x_test = []\n",
    "for review in test_data['review']:\n",
    "    x_test.append(' '.join(review_to_text(review,True)))\n",
    "\n",
    "y_train = train_data['sentiment']\n",
    "# print(x_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:  1.7min finished\n",
      "gs_count.best_score_: 0.88216\n",
      "gs_count.best_params_: {'count_vec__binary': True, 'count_vec__ngram_range': (1, 2), 'mnb__alpha': 1}\n",
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:  1.3min finished\n",
      "gs_tfidf.best_score_: 0.88712\n",
      "gs_tfidf.best_params_: {'mnb__alpha': 0.1, 'tfidf_vec__binary': True, 'tfidf_vec__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pip_count = Pipeline([('count_vec',CountVectorizer(analyzer='word')),('mnb',MultinomialNB())])\n",
    "pip_tfidf = Pipeline([('tfidf_vec',TfidfVectorizer(analyzer='word')),('mnb',MultinomialNB())])\n",
    "\n",
    "params_count = {'count_vec__binary':[True,False],'count_vec__ngram_range':[(1,1),(1,2)],'mnb__alpha':[0.1,1,10.0]}\n",
    "params_tfidf = {'tfidf_vec__binary':[True,False],'tfidf_vec__ngram_range':[(1,1),(1,2)],'mnb__alpha':[0.1,1,10.0]}\n",
    "\n",
    "gs_count = GridSearchCV(pip_count,params_count,cv=4,n_jobs=-1,verbose=1)\n",
    "gs_count.fit(x_train,y_train)\n",
    "print('gs_count.best_score_:',gs_count.best_score_)\n",
    "print('gs_count.best_params_:',gs_count.best_params_)\n",
    "count_y_predict = gs_count.predict(x_test)\n",
    "\n",
    "gs_tfidf = GridSearchCV(pip_tfidf,params_tfidf,cv=4,n_jobs=-1,verbose=1)\n",
    "gs_tfidf.fit(x_train,y_train)\n",
    "print('gs_tfidf.best_score_:',gs_tfidf.best_score_)\n",
    "print('gs_tfidf.best_params_:',gs_tfidf.best_params_)\n",
    "tfidf_y_predict = gs_tfidf.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_count = pd.DataFrame({'id':test_data['id'],'sentiment':count_y_predict})\n",
    "submission_tfidf = pd.DataFrame({'id':test_data['id'],'sentiment':tfidf_y_predict})\n",
    "\n",
    "submission_count.to_csv('datasets/submission_count.csv',index=False)\n",
    "submission_tfidf.to_csv('datasets/submission_tfidf.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "corpora:\n [['watching', 'time', 'chasers', 'it', 'obvious', 'that', 'it', 'was', 'made', 'by', 'a', 'bunch', 'of', 'friends'], ['maybe', 'they', 'were', 'sitting', 'around', 'one', 'day', 'in', 'film', 'school', 'and', 'said', 'hey', 'let', 's', 'pool', 'our', 'money', 'together', 'and', 'make', 'a', 'really', 'bad', 'movie', 'or', 'something', 'like', 'that'], ['what', 'ever', 'they', 'said', 'they', 'still', 'ended', 'up', 'making', 'a', 'really', 'bad', 'movie', 'dull', 'story', 'bad', 'script', 'lame', 'acting', 'poor', 'cinematography', 'bottom', 'of', 'the', 'barrel', 'stock', 'music', 'etc'], ['all', 'corners', 'were', 'cut', 'except', 'the', 'one', 'that', 'would', 'have', 'prevented', 'this', 'film', 's', 'release'], ['life', 's', 'like', 'that'], ['i', 'saw', 'this', 'film', 'about', 'years', 'ago', 'and', 'remember', 'it', 'as', 'being', 'particularly', 'nasty'], ['i', 'believe', 'it', 'is', 'based', 'on', 'a', 'true', 'incident', 'a', 'young', 'man', 'breaks', 'into', 'a', 'nurses', 'home', 'and', 'rapes', 'tortures', 'and', 'kills', 'various', 'women', 'it', 'is', 'in', 'black', 'and', 'white', 'but', 'saves', 'the', 'colour', 'for', 'one', 'shocking', 'shot', 'at', 'the', 'end', 'the', 'film', 'seems', 'to', 'be', 'trying', 'to', 'make', 'some', 'political', 'statement', 'but', 'it', 'just', 'comes', 'across', 'as', 'confused', 'and', 'obscene', 'avoid'], ['minor', 'spoilersin', 'new', 'york', 'joan', 'barnard', 'elvire', 'audrey', 'is', 'informed', 'that', 'her', 'husband', 'the', 'archeologist', 'arthur', 'barnard', 'john', 'saxon', 'was', 'mysteriously', 'murdered', 'in', 'italy', 'while', 'searching', 'an', 'etruscan', 'tomb'], ['joan', 'decides', 'to', 'travel', 'to', 'italy', 'in', 'the', 'company', 'of', 'her', 'colleague', 'who', 'offers', 'his', 'support'], ['once', 'in', 'italy', 'she', 'starts', 'having', 'visions', 'relative', 'to', 'an', 'ancient', 'people', 'and', 'maggots', 'many', 'maggots']]\n530086\n"
     ]
    }
   ],
   "source": [
    "unlabeled_data = pd.read_csv('datasets/unlabeledTrainData.tsv',delimiter='\\t',quoting=3)        #quoting=3   如实读取引号内容\n",
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences = []\n",
    "\n",
    "def new2sentences(review,sentences):\n",
    "    news_text = BeautifulSoup(review).get_text()\n",
    "    # print('news_text:\\n',news_text)\n",
    "    raw_sentences = tokenizer.tokenize(news_text)\n",
    "    # print('raw_sentences:\\n',raw_sentences)\n",
    "    for raw_sentence in raw_sentences:\n",
    "        sentences.append(re.sub('[^a-zA-Z]',' ',raw_sentence.lower().strip()).split())\n",
    "    return\n",
    "\n",
    "for review in unlabeled_data['review']:\n",
    "    # print('review:\\n',review)\n",
    "    new2sentences(review,sentences)\n",
    "\n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "# news = fetch_20newsgroups(subset='all')\n",
    "# x,y = news.data,news.target\n",
    "# for review in x:\n",
    "#     new2sentences(review,sentences)\n",
    "\n",
    "print('sentences:\\n',sentences[0:10])\n",
    "print(len(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('yahoo', 0.7360875606536865), ('mail', 0.6054712533950806), ('http', 0.5840326547622681), ('trust', 0.5825950503349304), ('com', 0.5734444856643677), ('woe', 0.5710731148719788), ('www', 0.5508212447166443), ('blogspot', 0.5481081008911133), ('atheist', 0.5228750109672546), ('grabbed', 0.522038459777832)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "num_features = 300\n",
    "min_word_count = 20\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "\n",
    "model = word2vec.Word2Vec(sentences,workers=num_workers, \\\n",
    "    size=num_features, min_count=min_word_count, \\\n",
    "    window=context, sample=downsampling)\n",
    "model.init_sims(replace=True)\n",
    "model_name = 'datasets/300feature_20minwords_10context'\n",
    "print(model.most_similar('email'))\n",
    "model.save(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('woman', 0.6411173343658447), ('lad', 0.6086446642875671), ('lady', 0.5957801342010498), ('person', 0.5326814651489258), ('men', 0.5281357169151306), ('chap', 0.5262560844421387), ('boy', 0.523442268371582), ('soldier', 0.5227000713348389), ('guy', 0.5177127122879028), ('monk', 0.5101040601730347)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(model_name)\n",
    "print(model.most_similar('man'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def make_feature_vec(words,model,num_features):\n",
    "    feature_vec = np.zeros(num_features,dtype='float32')\n",
    "    n_words = 0.0\n",
    "    index2words_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2words_set:\n",
    "            n_words += 1\n",
    "            feature_vec = feature_vec + model[word]\n",
    "    feature_vec = np.divide(feature_vec,n_words)\n",
    "    return feature_vec\n",
    "\n",
    "# 将每一条影评转换为基于词向量的特征向量\n",
    "def get_average_feature_vec(reviews,model,num_features):\n",
    "    counter = 0\n",
    "    review_feature_vecs = np.zeros([len(reviews),num_features],dtype='float32')\n",
    "    for review in reviews:\n",
    "        review_feature_vecs[counter] = make_feature_vec(review,model,num_features)\n",
    "        counter += 1\n",
    "    return review_feature_vecs\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in train_data['review']:\n",
    "    clean_train_reviews.append(review_to_text(review,remove_stopwords=True))\n",
    "train_data_vecs = get_average_feature_vec(clean_train_reviews,model,num_features)\n",
    "clean_test_reviews = []\n",
    "for review in test_data['review']:\n",
    "    clean_test_reviews.append(review_to_text(review,remove_stopwords=True))\n",
    "test_data_vecs = get_average_feature_vec(clean_test_reviews,model,num_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed: 18.1min finished\n",
      "gs.best_score_: 0.85808\n",
      "gs.best_params_: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 500}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier()\n",
    "# params_gbc = {'n_estimators':[10,100,500],'learning_rate':[0.01,0.1,1.0],'max_depth':[2,3,4]}\n",
    "params_gbc = {'n_estimators':[500],'learning_rate':[0.1],'max_depth':[4]}\n",
    "gs = GridSearchCV(gbc,params_gbc,cv=4,n_jobs=-1,verbose=1)\n",
    "gs.fit(train_data_vecs,y_train)\n",
    "print('gs.best_score_:',gs.best_score_)\n",
    "print('gs.best_params_:',gs.best_params_)\n",
    "v2c_y_predict=gs.predict(test_data_vecs)\n",
    "submission_v2c = pd.DataFrame({'id':test_data['id'],'sentiment':v2c_y_predict})\n",
    "submission_v2c.to_csv('datasets/submission_v2c.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}